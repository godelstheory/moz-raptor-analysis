---
title: 'Visual Metrics And Navigation Timings: RUMSpeedIndex and Network Metrics'
author: "Corey Dow-Hygelund"
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---

```{r library_imports, message=FALSE, warning=FALSE, echo=FALSE}
library(corrplot)
library(dplyr)
library(ggplot2)
library(tidyr)
library(funModeling)
library(cowplot)
library(ggridges)
library(Boruta)
library(viridis)
library(caret)
library(stringr)
library(plotly)
library(MLmetrics)
```

# tl;dr
This analysis is an extension of the [previous](https://metrics.mozilla.com/protected/cdowhygelund/visual_metrics_nav_timings.html), with the inclusion of additional covariates for modeling the visual metrics, specifically resource and network metrics. Its focus is to determine an optimal linear regression model that predicts `speedIndex` using the navigation and resource metrics. It is found the relationship between the visual and metrics has a strong dependency on URL, but a weak on mode (e.g., application). The resultant model performance is < > from the training set, and < > using a hold-out validation set. 

This work establishes the method of producing a production model. To obtain a production model in practice, it is recommended that significantly more URLs are measured across a range of applications and platforms. 

# Datasets{.tabset}

Two datasets were provided in early 2020 by Andrew Creskey that contain in-house, live-site testing of a range of URLs for various mobile Firefox versions (`mode`) . Hereafter they referred to as drop 1 and 2. The datasets have identical measurements, save the inclusion of `rumSpeedIndex` in the drop 2. However, they differ on the mobile application version, and URLs tested.


```{r data_import, echo=FALSE, warning=FALSE}
load('livesites_withNetworkMetrics.RData')
df_0120 <- df 

load('data_rumSpeedIndex.RData')
df_0220 <- df 

df <- df_0220 %>%
  bind_rows(df_0120) %>%
  mutate(mode = as.factor(mode)) %>%
  mutate(url = as.factor(url))

visual_metrics <- sort(
  c(
    'firstVisualChange',
    'visualComplete85',
    'speedIndex',
    'contentfulSpeedIndex',
    'perceptualSpeedIndex'
    )
)

nav_timings <- sort(names(df)[!names(df) %in% c('mode', 'url', visual_metrics)])

df_0220 <- df_0220 %>% 
  filter(DOMContentLoaded < 10000)

df_0220_long <- df_0220 %>%
  gather('vis_metric', 'vis_metric_value', -nav_timings, -mode, -url) %>%
  gather('nav_timing', 'nav_timing_value', -vis_metric, -vis_metric_value, -mode, -url)

urls <- unique(df_0220$url)
```

The same number of measurements with respect to URL and `mode` were taken relative to each dataset:

* Drop 1: `r df_0120 %>% count(mode, url) %>% pull(n) %>% max()` per URL and mode
* Drop 2: `r df_0220 %>% count(mode, url) %>% pull(n) %>% max()` per URL and mode

## Mode

The `mode` field represents the Fenix, Fennec, or GeckoView version tested. 

```{r mode, echo=FALSE}
knitr::kable(data.frame(Mode = sort(unique(df$mode))) %>%
               mutate(`Drop 1` = Mode %in% df_0120$mode) %>%
               mutate(`Drop 2` = Mode %in% df_0220$mode) )
```

## URLs
A total of `r length(unique(df$url))` URLs were measured between the two drops, with `r length(unique(df_0120$url))` being measured in the first, and `r length(unique(df_0220$url))` in the second. 


```{r urls, echo=FALSE}
knitr::kable(
  data.frame(URL = sort(unique(df$url))) %>%
    mutate(`Drop 1` = URL %in% df_0120$url) %>%
    mutate(`Drop 2` = URL %in% df_0220$url) 
    )
```

## Visual Metrics
`r length(visual_metrics) were measured.

```{r vis_metrics, echo=FALSE}
knitr::kable(df_status(df %>% select(visual_metrics), print_results=FALSE))
```

## Navigation and Resource Metrics

The [navigation](https://docs.google.com/document/d/1W-EREsJLuRvTPvGXaW71FvuAGXkoLNDZmZl-sbaeMZM/edit#heading=h.q9b66ketjl7j) and resource metrics can be categorized as follows:

Features that represent page load completion:

* `loadtime`
* `rumSpeedIndex`
* `domInteractive`
* `DOMContentLoaded`
* `firstPaint`

Feature representing starting point of backend activity:

* `connectStart`
* `domainLookupStart`
* `domainLookupEnd`
* `fetchStart`
* `requestStart`
* `responseStart`
* `responseEnd`

Features representing total duration of certain portions of backend activity:

* `backendTime`
* `domainLookupTime`
* `frontEndTime`
* `redirectionTime`
* `resourceDuration`
* `serverConnectionTime`
* `serverResponseTime`

Features representing resource usage:

* `decodedBodySize`
* `encodedBodySize`
* `resourceCount`

```{r nav_timing_grps, echo=FALSE}
pl_covars <- c('loadtime', 'rumSpeedIndex', 'domInteractive', 'DOMContentLoaded', 'firstPaint')
bckend_covars <- c('connectStart', 'domainLookupStart', 'domainLookupEnd', 'fetchStart', 
                   'requestStart', 'responseStart', 'responseEnd')
dur_covars <- c('backendTime', 'frontEndTime', 'resourceDuration', 'serverResponseTime')
res_covars <- c('decodedBodySize', 'encodedBodySize','resourceCount')
```

```{r filtered_nav_timing, echo=FALSE}
df_state <- df_status(df %>% select(nav_timings), print_results=FALSE)

dropped_covars <- df_state %>%
  filter(q_zeros > 0) %>%
  pull(variable)

nav_timing_clean <- nav_timings[!nav_timings %in% dropped_covars]
```

`r length(nav_timings)-1` navigation and resource metrics were measured for each drop. `r length(dropped_covars)` had a significant fraction of 0 values, correspoding to no measurement. Interstingly, these all appear to be total times <need better explaination>. These have been filtered from subsequent analysis (`r paste(dropped_covars, sep=', ')`).

```{r args, echo=FALSE}
knitr::kable(df_state)
```

# SpeedIndex{.tabset}
Focus on drop 2, as each mode was measured for each URL (e.g., completeness). 

## Mode

The following plot shows the distribution of speedIndex, in two different views ridge (left) and violin (right) plots. The shading in the ridge plots represent the 0-20th, 20-40th, 40-60th, and 80-100th quintiles 

The distributions between builds of Fenix are very consistent. `fennec68` has a lower values for the lower two quintiles, and higher for the 5th. 

```{r speedIndex_d2, fig.width=8, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE}
response <- 'speedIndex'

df_0220_long_si <- df_0220 %>%
  select(mode, url, speedIndex, nav_timing_clean) %>%
  gather('nav_timing', 'nav_timing_value', -speedIndex, -mode, -url)

# ps = list()
# ps[['ridge']] <- 
ggplot(df_0220, aes(x=speedIndex, y=mode, fill = factor(stat(quantile)))) +
  stat_density_ridges(
    geom = "density_ridges_gradient", calc_ecdf = TRUE,
    quantiles = 5, quantile_lines = TRUE
  ) +
  scale_fill_viridis_d(name = "Quintiles") +
  theme(axis.text.y = element_text(angle = 25, hjust = 1)) +
  xlim(0, 10000)

# ps[['violin']] <- ggplot(df_0220, aes(mode, speedIndex)) + 
#   geom_violin(draw_quantiles = c(0.20, 0.4, 0.60, 0.80)) +
#   theme(axis.text.x = element_text(angle = 20, hjust = 1))
# 
# plot_grid(plotlist = ps)
```

## Correlation {#corr}

```{r corr, echo=FALSE}
df_corr <- df_0220[, c(nav_timing_clean, 'speedIndex')]
cor_si <- cor(df_corr)

cor_si_cut <- as.data.frame(cor_si) %>% 
  tibble::rownames_to_column("nav_timing") %>% 
  select(nav_timing, speedIndex) %>% 
  filter(speedIndex > 0.5) %>% 
  arrange(desc(speedIndex)) %>%
  slice(-1)
```

The correlation of these visual metrics across all modes is shown in the following plot. The rightmost column contains `speedIndex`. `r length(cor_si_cut$nav_timing)` timings have correlations > 0.5: `r paste(cor_si_cut$nav_timing, sep=',')`. Interestingly, `rumSpeedIndex` has a low Pearson correlation with `speedIndex`.

A few immediate takeaways: 

*  For starting point timings:
    - `responseStart` and `responseEnd` are perfectly correlated to `backendTime`. 
    - `connectStart`, `domainLookupEnd`, `domainLookupStart`, `fetchStart` as perfectly correlated to `redirectionTime`.
*  For page load metrics
    - `loadtime` correlates the strongest with `speedIndex`.
    - `rumSpeedIndex` and `loadtime` have least correlation.


```{r corrplot, fig.width=10, fig.height=10, echo=FALSE, warning=FALSE}
corrplot::corrplot(cor_si, 
                   method = 'pie',
                   type = 'upper')# , order = 'hclust', addrect = 3) 
# corrplot(cor(df %>% select(-c(mode, url))), method = 'pie', type = 'upper') 
```

## Navigation and Resource Metrics{.tabset}
{#nav}
The following interactive plots display `speedIndex` versus the navigation and resource metrics. The color represents URL, and shape represents mode. There are a few clear takeaways:

* Relationships are strongly dependent upon `uri`.
* Relationships have much less dependency on `mode`.
   - However, `fennec68` does have cases were it is offset with respect to `fenix<>` measurements. 
* Several of the metris, especially the page load timings, have a strong linear relationship with `speedIndex` (e.g., `rumSpeedIndex`, `loadTime`, `encodedBodySize`),
  - However, there are consistent outliers across the metrics (e.g.: https://www.nytimes.com, https://www.bing.com/search?q=restaurant).
* `decodedBodySize` and `encodedBodySize` cannot account simultaneoulsy for the deviation in https://www.nytimes.com, and the large spread in https://www.bing.com/search?q=restaurant, with high values observed for https://accounts.google.com.
* Significant spread in the relationship to navigation and resource metrics for low `speedIndex` values. 

### Page Load Timings

The figures below compares `speedIndex` to the page load timings. 

```{r , fig.width=15, fig.height=40, echo=FALSE, warning=FALSE}

# ggplot(df_0220_long %>% 
#          filter(vis_metric == 'speedIndex'), 
#        aes(vis_metric_value, nav_timing_value)) +
#   geom_point(aes(shape=mode, color=url)) + 
#   facet_grid(vars(nav_timing), scales='free')

# plist <- list()
# for (i in 1:length(nav_timing_clean)){
#   nav_timing <- nav_timing_clean[i]
#   p <- ggplot(df_0220 %>%
#                 filter(DOMContentLoaded < 10000) %>%
#                 filter(resourceDuration < 500000),
#               aes_string(nav_timing, 'speedIndex')) +
#     geom_point(aes(color = url, shape=mode)) +
#     geom_smooth(method = "lm", se = FALSE) +
#     theme_bw()
#   if(i==22){
#     # plist[['legend']] <- get_legend(p + theme(legend.position = "top"))
#     plist[['legend']] <- get_legend(p + theme(legend.position = "right"))
#   }
#   p <- p + theme(legend.position = "none")
#   plist[[nav_timing]] <- p
# }
# 
# plot_grid(plotlist = plist, ncol=3)

# plist <- list()
# covar_grps <- split(nav_timing_clean, ceiling(seq_along(nav_timing_clean)/3))
# for (i in 1:length(covar_grps)){
#   grp <- covar_grps[i][[1]]
#   df_covar_grp <- df_0220_long %>%
#     filter(nav_timing %in% grp) %>%
#     filter(vis_metric == 'speedIndex')
#   p <- ggplot(df_covar_grp, aes(nav_timing_value, vis_metric_value)) +
#     geom_point(aes(color = url, shape=mode)) +
#     geom_smooth(method = "lm", se = FALSE) +
#     facet_grid(cols=vars(nav_timing), scales = 'free') +
#     theme_bw()
#   if(i==22){
#     # plist[['legend']] <- get_legend(p + theme(legend.position = "top"))
#     plist[['legend']] <- get_legend(p + theme(legend.position = "right"))
#   }
#   p <- p + theme(legend.position = "none")
#   plist[[i]] <- p
# }
# 
# plot_grid(plotlist = plist, ncol=1)
```

```{r page_load_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
# out.width='100%', out.height='100%', echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% pl_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') +
  ylab('SpeedIndex'))  #%>%
  #layout(autosize = F, autosize = F, width = 800, height = 500)
```

### Backend Timings

The figures below compares `speedIndex` to the starting point of backend activity timings. 

```{r backend_timing_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% bckend_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') + 
  ylab('SpeedIndex')) #%>%
  #layout(autosize = F, autosize = F, width = 800, height = 500)
```

### Backend Durations

The figures below compares `speedIndex` to the duration of background activity timings. 

```{r backend_dur_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% dur_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') + 
  ylab('SpeedIndex')) # %>%
  #layout(autosize = F, autosize = F, width = 800, height = 500)
```

### Resource Usage

The figures below compares `speedIndex` to resource usage metrics. 

```{r resource_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% res_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') + 
  ylab('SpeedIndex')) # %>%
  # layout(autosize = F, autosize = F, width = 800, height = 500)
```

# Process
The modeling effort focused on best representing `speedIndex` using the navigation and resource metrics for the 2nd dataset drop containing `rumSpeedIndex`. The steps taken are as follows: 

1. Review correlations between fields and `speedIndex`.
2  Generate new features that could be informative. 
3. Drop uninformative and unimportant features.
4. Perform feature selection to narrow covariate space. Keep top four features. 
5. Perform leave-one-out cross validation with respect to unique URI. Simple linear models are trained. 
6. Validate model performance against the first drop dataset. 

## Feature Filtering
Redundant and uninformative features were filtered, in order to increase model performance, and to keep the feature selection search space tractable. The following filtering steps were performed:

* `responseStart` and `responseEnd` were dropped as they are perfectly correlated to `backendTime`.
* `connectStart`, `domainLookupEnd`, `domainLookupStart`, `fetchStart` were dropped as they are perfectly correlated to `redirectionTime`.
* The page load timings `domInteractive`, `DOMContentLoaded`, `firstPaint` were filtered:
    - `loadtime` correlates strongest with `speedIndex`.
    - `rumSpeedIndex` has good modeling properties on visual inspection.
    - `rumSpeedIndex` and `loadtime` have least correlation between page load metrics
* `decodedBodySize` is dropped as it is very similar to `encodedBodySize`. 


##  Feature Generation

Features were generated using [page load cycle](https://docs.google.com/document/d/1W-EREsJLuRvTPvGXaW71FvuAGXkoLNDZmZl-sbaeMZM/edit#heading=h.q9b66ketjl7j) definitions,  referring to efforts of this [bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1615369), and the results of the [correlation](#corr) and [exploratory](#nav) analyses. The general idea was to focus on the page load covariates, and use the remaining timings, durations, and resource usage metrics as corrections to the model. 

* Ratio of `decodedBodySize` to `encodedBodySize`: 
    - `ratioDeEnCoded`
* Ratio of `resourceCount` to `encodedBodySize`: 
    - `ratioRescountEncoded`
* Ratio of `loadtime` to `resourceCount` and `encodedBodySize`: 
    - `lt_resourceCount`, `lt_encodedBodySize`
* Ratio of `rumSpeedIndex` to `resourceCount` and `encodedBodySize`: 
    - `rsi_resourceCount`, `rsi_encodedBodySize`
* Difference of `responseStart`, `requestStart` to `loadtime`:
    - `lt_responseStart`, `lt_requestStart`
* Difference of `responseStart`, `requestStart` to `rumSpeedIndex`:
    - `rsi_responseStart`
    - `rsi_requestStart`
* ratio of `loadtime` to durations:
    - `lt_backendTime`, `lt_frontEndTime`, `lt_resourceDuration`, `lt_serverResponseTime`
* ratio of `rumSpeedIndex` to durations:
    - `rsi_backendTime`, `rsi_frontEndTime`, `rsi_resourceDuration`, `rsi_serverResponseTime`
* ratio of durations to `encodedBodySize`:
  - `bet_encodedBodySize`, `fet_encodedBodySize`, `rd_encodedBodySize`, `srt_encodedBodySize`
  - `encodedBodySize` is not highly correlated to any durations.

```{r feature_generation, echo=FALSE}

df_0220_ultra <- df_0220 %>% 
  # ratio of de to encodedBodySize
  mutate(ratioDeEnCoded = decodedBodySize/encodedBodySize) %>%
  # ratio of resourceCount to encodedBodySize
  mutate(ratioRescountEncoded = resourceCount/encodedBodySize) %>%
  # ratio of page load to encodedBodySize
  mutate(rsi_encodedBodySize = rumSpeedIndex/encodedBodySize) %>% 
  mutate(rsi_resourceCount = rumSpeedIndex/resourceCount) %>%
  mutate(lt_encodedBodySize = loadtime/encodedBodySize) %>% 
  mutate(lt_resourceCount = loadtime/resourceCount) %>%
  # difference of responseStart
  mutate(rsi_responseStart = rumSpeedIndex-responseStart) %>% 
  mutate(rsi_requestStart = rumSpeedIndex-requestStart) %>%
  mutate(lt_responseStart = loadtime-responseStart) %>% 
  mutate(lt_requestStart = loadtime-requestStart) %>%
  # ratio of page load to durations 
  mutate(rsi_backendTime = rumSpeedIndex/backendTime) %>% 
  mutate(rsi_frontEndTime = rumSpeedIndex/frontEndTime) %>% 
  mutate(rsi_resourceDuration = rumSpeedIndex/resourceDuration) %>%
  mutate(rsi_serverResponseTime = rumSpeedIndex/serverResponseTime) %>%
  mutate(lt_backendTime = loadtime/backendTime) %>% 
  mutate(lt_frontEndTime = loadtime/frontEndTime) %>% 
  mutate(lt_resourceDuration = loadtime/resourceDuration) %>%
  mutate(lt_serverResponseTime = loadtime/serverResponseTime) %>%
  # ratio of durations to encodedBodySize
  mutate(bet_encodedBodySize = backendTime/encodedBodySize) %>% 
  mutate(fet_encodedBodySize = frontEndTime/encodedBodySize) %>%
  mutate(rd_encodedBodySize = resourceDuration/encodedBodySize) %>%
  mutate(srt_encodedBodySize = serverResponseTime/encodedBodySize) 
```

```{r feature_filtering, echo=FALSE}
df_0220_ultra_vis <- df_0220_ultra %>%
  select(-c(responseStart, responseEnd, connectStart, domainLookupStart, domainLookupEnd, fetchStart,
         domInteractive, DOMContentLoaded, firstPaint, 
         decodedBodySize))

df_0220_ultra_f <- df_0220_ultra_vis %>%
  select(-c(visualComplete85, firstVisualChange, contentfulSpeedIndex, perceptualSpeedIndex)) 
         
```


# Variable Importance{.tabset}

To limit the numbrer of covariates utilized in model fitting to prevent overfitting, feature selection was performed on the complete covariate set. There are significant number of methods for determining variable importance. Two explored here are using [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline) and and the R library Boruta.

<Explain resultant feature defs>

## Mars
Multiple adaptive regression splines (MARS) is a form of regression that is an extension of multiple linear regressions and generalized linear models.  This algorithm can be used for variable importance, as the  contribution  (e.g., variable importance  score) for each covariate can be calculated using generalized cross-validation (GCV) statistics.The table below shows the selected covariates after feature selection by MARS. 
```{r mars, echo=FALSE, warning=FALSE, message=FALSE}
library(earth)
speedIndex.mars <- earth(speedIndex ~ ., data = df_0220_ultra_f %>% 
                           select(-c(mode, url)))
ev <- evimp (speedIndex.mars)
knitr::kable(as.data.frame(unclass(ev[,c(3,4,6)])) %>%
               tibble::rownames_to_column(., "covariate") %>%
               select(covariate, gcv)
             )
# plot(ev)
```

## Boruta
[Boruta](https://cran.r-project.org/web/packages/Boruta/index.html) is a feature selection algorithm that trains suites of Random Forest models to determine variable importance for regression and classification tasks. The following table shows the variable importance measures found for each navigation timing.  

<explain table> 
```{r boruta, echo=FALSE, warning=FALSE, message=FALSE}
speedIndex.bor <- Boruta(speedIndex ~ ., data = df_0220_ultra_f %>% 
                           select(-c(mode, url)),
                         doTrace = TRUE, ntree = 500, maxRuns = 1000)

speedIndex.bor.df <- attStats(speedIndex.bor) %>% 
  tibble::rownames_to_column(., 'covariate') %>% 
  arrange(desc(medianImp)) %>%
  select(covariate, meanImp, medianImp)

knitr::kable(speedIndex.bor.df)
```



# Modeling{.tabset}

[Leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) was performed across the URLs to determine model performance. The mean absolute percentage error loss (MAPE) was calculated from the resultant predictions for each help out URL.  

### Mars Features

```{r lm_loo, echo=FALSE, warning=FALSE}
cv_lm_loo <- function(df, formula, response){
  cv <- NULL
  for (i in 1:length(urls)){
    # train/test samples
    train_urls <- urls[-i]
    train <- df %>% 
      filter(url %in% train_urls) 
    test <- df %>% 
      filter(!url %in% train_urls)
    # fit model
    lm_cv <- lm(formula = formula, data = train) 
    cv <- rbind(cv, data.frame(response = test[[response]],
                             prediction = predict(lm_cv, test),
                             fold = i,
                             url = test$url,
                             mode = test$mode))
  }
  # cv <- cv %>%
  #   mutate(residual = (prediction-response)/response)
  return(cv)
}

cv <- cv_lm_loo(df_0220_ultra_f, 
                      formula = speedIndex ~ rumSpeedIndex + ratioDeEnCoded + lt_encodedBodySize + lt_backendTime,
                      response) %>%
  gather('metric', 'value', -response, -url, -mode, -fold)

mars_mape = MAPE(cv$value, cv$response)
mars_r2 = R2()
```

CV was performed on a model trained with the top 4 features found by MARS, with a final MAPE and R2 scores of `r MAPE(cv$value, cv$response)` and `r R2(cv$value, cv$response)`.

```{r lm_loo_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(cv, aes(value, response))+
  geom_point(aes(color = url, shape=mode)) +
  geom_abline(slope = 1) + 
  geom_abline(slope = 0) + 
  # facet_grid(rows = vars(metric), scales = 'free_y') +
  theme_bw() + 
  theme(legend.position = "none")+
        # axis.text.x = element_text(angle = 90, hjust = 1)) +
  #guides(color = guide_legend(ncol=2, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  ylab(response) +
  xlab('Predictions')
  )
```

### Boruta Features
```{r lm_loo_boruta, echo=FALSE, warning=FALSE}
cv <- cv_lm_loo(df_0220_ultra_f, 
                      formula = speedIndex ~ rumSpeedIndex + rsi_requestStart + srt_encodedBodySize + loadtime + rsi_responseStart,
                      response) %>%
    gather('metric', 'value', -response, -url, -mode, -fold)
```

CV was performed on a model trained with the top 4 features found by Boruta, with a final MAPE and R2 scores of `r MAPE(cv$value, cv$response)` and `r R2(cv$value, cv$response)`. Interestingly, Boruta's selected feature perform significantly worse than MARS, which could be due to using decision trees instead of linear regression models. 

```{r lm_loo_boruta_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(cv, aes(value, response)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_abline(slope = 1) + 
  geom_abline(slope = 0) + 
  # facet_grid(rows = vars(metric), scales = 'free_y') +
  theme_bw() + 
  theme(legend.position = "none")+
        # axis.text.x = element_text(angle = 90, hjust = 1)) +
  #guides(color = guide_legend(ncol=2, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  ylab(response) +
  xlab('Predictions')
  )
```


# Model Validation{.tabset}

The final modeling step is to calculate model performance using a held-out dataset. For this purposes the drop 1 dataset was used.

## Model Fit
The drop 1 dataset doesn't contain `rumSpeedIndex`, which is one of top 4 features found by MARS. Therefore, feature selection was performed again, subtracting this feature and any of its derivatives:

The results of the MARS algorithm:
```{r mars_val, echo=FALSE, warning=FALSE, error=FALSE}
library(earth)
speedIndex.mars.val <- earth(speedIndex ~ ., data = df_0220_ultra_f %>% 
                           select(-c(mode, url, rumSpeedIndex, starts_with('rsi'))))
ev.val <- evimp (speedIndex.mars.val)
knitr::kable(as.data.frame(unclass(ev.val[,c(3,4,6)])) %>%
               tibble::rownames_to_column(., "covariate") %>%
               select(covariate, gcv)
             )
```


```{r loo_cv_val, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
cv <- cv_lm_loo(df_0220_ultra_f, 
                      formula = speedIndex ~ loadtime + ratioDeEnCoded + lt_encodedBodySize + resourceCount,
                      response) %>%
    gather('metric', 'value', -response, -url, -mode, -fold)

```

LOO CV using these features yeilds a MAPE and R2 scores of `r MAPE(cv$value, cv$response)` and `r R2(cv$value, cv$response)`, respectively. The MAPE score is higher than that found with features including `rumSpeedIndex`, suggesting weaker model performance and a poorer resultant fit. 

```{r loo_cv_val_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(cv, aes(value, response)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_abline(slope = 1) + 
  geom_abline(slope = 0) + 
  # facet_grid(rows = vars(metric), scales = 'free_y') +
  theme_bw() + 
  theme(legend.position = "none")+
        # axis.text.x = element_text(angle = 90, hjust = 1)) +
  #guides(color = guide_legend(ncol=2, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  ylab(response) +
  xlab('Predictions')
  )
```

## Validation

```{r lm_val_model_fit, echo=FALSE, warning=FALSE}
lm.val <- lm(speedIndex ~ loadtime + ratioDeEnCoded + lt_encodedBodySize + resourceCount, data = df_0220_ultra_f)

df.val <- df_0120 %>%
  # ratio of de to encodedBodySize
  mutate(ratioDeEnCoded = decodedBodySize/encodedBodySize) %>%
  # ratio of page load to encodedBodySize
  mutate(lt_encodedBodySize = loadtime/encodedBodySize) %>% 
  mutate(lt_resourceCount = loadtime/resourceCount) %>%
  # difference of responseStart
  mutate(lt_responseStart = loadtime-responseStart) %>% 
  mutate(lt_requestStart = loadtime-requestStart) %>%
  # ratio of page load to durations 
  mutate(lt_backendTime = loadtime/backendTime) %>% 
  mutate(lt_frontEndTime = loadtime/frontEndTime) %>% 
  mutate(lt_resourceDuration = loadtime/resourceDuration) %>%
  mutate(lt_serverResponseTime = loadtime/serverResponseTime) %>%
  # ratio of durations to encodedBodySize
  mutate(bet_encodedBodySize = backendTime/encodedBodySize) %>% 
  mutate(fet_encodedBodySize = frontEndTime/encodedBodySize) %>%
  mutate(rd_encodedBodySize = resourceDuration/encodedBodySize) %>%
  mutate(srt_encodedBodySize = serverResponseTime/encodedBodySize) %>%
  mutate(predictions = predict(lm.val, .))
```

The validation MAPE and R2 scores are `r MAPE(df.val$predictions, df.val$speedIndex)` and `r R2(df.val$predictions, df.val$speedIndex)`, respectively. There is a strong dependency of model performance with URL, where https://imgur.com is not adequately explained by the model. 

```{r lm_val_model_fit_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}

ggplotly(
    ggplot(df.val, aes(predictions, speedIndex))+
    geom_point(aes(color = url, shape=mode)) +
    geom_abline(slope = 1) + 
    geom_abline(slope = 0) + 
    # facet_grid(rows = vars(metric), scales = 'free_y') +
    theme_bw() + 
    theme(legend.position = "none") +
    ylab(response) + 
      xlab('predictions')
)
```

# Visual Metrics{.tabset}

## `rumSpeedIndex` Features
```{r vis_met_comp_1, fig.width=10, fig.height=10, echo=FALSE}
df_0220_ultra_vis_long <- df_0220_ultra_vis %>%
  gather('vis_metric', 'vis_metric_value', visual_metrics) %>%
  gather('nav_timing', 'nav_timing_value', -vis_metric, -vis_metric_value, -mode, -url)

ggplotly(
  ggplot(df_0220_ultra_vis_long %>%
                filter(nav_timing %in% rownames(ev[1:4, ])),
                aes(nav_timing_value, vis_metric_value)) +
    facet_grid(vars(vis_metric), vars(nav_timing), scales='free') + 
    geom_point(aes(color = url, shape=mode)) +
    geom_smooth(method = "lm", se = FALSE) + 
    theme_bw() + 
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 90, hjust = 1)) +
    guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
    xlab('Navigation Timing Value') + 
    ylab('Visual Metric Value')
)
```

## `loadtime` Features
```{r vis_met_comp_2, fig.width=10, fig.height=10, echo=FALSE}
ggplotly(
  ggplot(df_0220_ultra_vis_long %>%
                filter(nav_timing %in% rownames(ev.val[1:4, ])),
                aes(nav_timing_value, vis_metric_value)) +
    facet_grid(vars(vis_metric), vars(nav_timing), scales='free') + 
    geom_point(aes(color = url, shape=mode)) +
    geom_smooth(method = "lm", se = FALSE) + 
    theme_bw() + 
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 90, hjust = 1)) +
    guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
    xlab('Navigation Timing Value') + 
    ylab('Visual Metric Value')
)
```

# Next Steps

1. Determine which visual metric to focus upon
1. Acquire significantly more live sites
2. Utilize RFE, with resampling across URLs to determine optimal feature set.
3. Examine additional regression modeling techinques.
4. Weighted regression to deal with URL splitting. Very little data in middel. 
3. Additional metrics to determine differences observed
   - specific URLs
   
# Cut
Questions?
* What covariates are in Webpagetest, relative to those used in model?

Focus:
* On second drop dataset. 
  - rumtimings helpful but not the best
* Webpagetest is useful how?

* data cleaning: note that there is an anomalous point at large domcontentloaded

* LOO best option: data is all clustered by URL. Therefore, need to CV around URL to get best estimation of model prediction performance. Otherwise, we are transfering information. 
