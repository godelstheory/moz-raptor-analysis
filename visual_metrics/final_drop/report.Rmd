---
title: 'Visual Metrics And Navigation Timings: RUMSpeedIndex and Network Metrics'
author: "Corey Dow-Hygelund"
date: 'Last Updated: `r format(Sys.time(), "%B %d, %Y")`'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---

```{r library_imports, message=FALSE, warning=FALSE, echo=FALSE}
library(corrplot)
library(dplyr)
library(ggplot2)
library(tidyr)
library(funModeling)
library(cowplot)
library(ggridges)
library(Boruta)
library(viridis)
library(caret)
library(stringr)
library(plotly)
library(MLmetrics)
```

```{r data_import, echo=FALSE, warning=FALSE}
response <- 'speedIndex'

load('livesites_withNetworkMetrics.RData')
df_0120 <- df 

load('data_rumSpeedIndex.RData')
df_0220 <- df 

df <- df_0220 %>%
  bind_rows(df_0120) %>%
  mutate(mode = as.factor(mode)) %>%
  mutate(url = as.factor(url))

visual_metrics <- sort(
  c(
    'firstVisualChange',
    'visualComplete85',
    'speedIndex',
    'contentfulSpeedIndex',
    'perceptualSpeedIndex'
    )
)

nav_timings <- sort(names(df)[!names(df) %in% c('mode', 'url', visual_metrics)])

df_0220 <- df_0220 %>% 
  filter(DOMContentLoaded < 10000)

df_0220_long <- df_0220 %>%
  gather('vis_metric', 'vis_metric_value', -nav_timings, -mode, -url) %>%
  gather('nav_timing', 'nav_timing_value', -vis_metric, -vis_metric_value, -mode, -url)

urls <- unique(df_0220$url)
```

```{r nav_timing_grps, echo=FALSE}
pl_covars <- c('loadtime', 'rumSpeedIndex', 'domInteractive', 'DOMContentLoaded', 'firstPaint')
bckend_covars <- c('connectStart', 'domainLookupStart', 'domainLookupEnd', 'fetchStart', 
                   'requestStart', 'responseStart', 'responseEnd')
dur_covars <- c('backendTime', 'frontEndTime', 'resourceDuration', 'serverResponseTime')
res_covars <- c('decodedBodySize', 'encodedBodySize','resourceCount')
```

```{r filtered_nav_timing, echo=FALSE}
df_state <- df_status(df %>% select(nav_timings), print_results=FALSE)

dropped_covars <- df_state %>%
  filter(q_zeros > 0) %>%
  pull(variable)

nav_timing_clean <- nav_timings[!nav_timings %in% dropped_covars]
```

```{r feature_generation, echo=FALSE}

df_0220_ultra <- df_0220 %>% 
  # ratio of de to encodedBodySize
  mutate(ratioDeEnCoded = decodedBodySize/encodedBodySize) %>%
  # ratio of resourceCount to encodedBodySize
  mutate(ratioRescountEncoded = resourceCount/encodedBodySize) %>%
  # ratio of page load to encodedBodySize
  mutate(rsi_encodedBodySize = rumSpeedIndex/encodedBodySize) %>% 
  mutate(rsi_resourceCount = rumSpeedIndex/resourceCount) %>%
  mutate(lt_encodedBodySize = loadtime/encodedBodySize) %>% 
  mutate(lt_resourceCount = loadtime/resourceCount) %>%
  # difference of responseStart
  mutate(rsi_responseStart = rumSpeedIndex-responseStart) %>% 
  mutate(rsi_requestStart = rumSpeedIndex-requestStart) %>%
  mutate(lt_responseStart = loadtime-responseStart) %>% 
  mutate(lt_requestStart = loadtime-requestStart) %>%
  # ratio of page load to durations 
  mutate(rsi_backendTime = rumSpeedIndex/backendTime) %>% 
  mutate(rsi_frontEndTime = rumSpeedIndex/frontEndTime) %>% 
  mutate(rsi_resourceDuration = rumSpeedIndex/resourceDuration) %>%
  mutate(rsi_serverResponseTime = rumSpeedIndex/serverResponseTime) %>%
  mutate(lt_backendTime = loadtime/backendTime) %>% 
  mutate(lt_frontEndTime = loadtime/frontEndTime) %>% 
  mutate(lt_resourceDuration = loadtime/resourceDuration) %>%
  mutate(lt_serverResponseTime = loadtime/serverResponseTime) %>%
  # ratio of durations to encodedBodySize
  mutate(bet_encodedBodySize = backendTime/encodedBodySize) %>% 
  mutate(fet_encodedBodySize = frontEndTime/encodedBodySize) %>%
  mutate(rd_encodedBodySize = resourceDuration/encodedBodySize) %>%
  mutate(srt_encodedBodySize = serverResponseTime/encodedBodySize) 
```

```{r feature_filtering, echo=FALSE}
df_0220_ultra_vis <- df_0220_ultra %>%
  select(-c(responseStart, responseEnd, connectStart, domainLookupStart, domainLookupEnd, fetchStart,
         domInteractive, DOMContentLoaded, firstPaint, 
         decodedBodySize))

df_0220_ultra_f <- df_0220_ultra_vis %>%
  select(-c(visualComplete85, firstVisualChange, contentfulSpeedIndex, perceptualSpeedIndex)) 
         
```

```{r mars, echo=FALSE, warning=FALSE, message=FALSE}
library(earth)
speedIndex.mars <- earth(speedIndex ~ ., data = df_0220_ultra_f %>% 
                           select(-c(mode, url)))
ev <- evimp (speedIndex.mars)
# plot(ev)
```

```{r lm_loo, echo=FALSE, warning=FALSE}
cv_lm_loo <- function(df, formula, response){
  cv <- NULL
  for (i in 1:length(urls)){
    # train/test samples
    train_urls <- urls[-i]
    train <- df %>% 
      filter(url %in% train_urls) 
    test <- df %>% 
      filter(!url %in% train_urls)
    # fit model
    lm_cv <- lm(formula = formula, data = train) 
    cv <- rbind(cv, data.frame(response = test[[response]],
                             prediction = predict(lm_cv, test),
                             fold = i,
                             url = test$url,
                             mode = test$mode))
  }
  # cv <- cv %>%
  #   mutate(residual = (prediction-response)/response)
  return(cv)
}

cv <- cv_lm_loo(df_0220_ultra_f, 
                      formula = speedIndex ~ rumSpeedIndex + ratioDeEnCoded + lt_encodedBodySize + lt_backendTime,
                      response) %>%
  gather('metric', 'value', -response, -url, -mode, -fold)

```

```{r lm_val_model_fit, echo=FALSE, warning=FALSE}
lm.val <- lm(speedIndex ~ loadtime + ratioDeEnCoded + lt_encodedBodySize + resourceCount, data = df_0220_ultra_f)

df.val <- df_0120 %>%
  # ratio of de to encodedBodySize
  mutate(ratioDeEnCoded = decodedBodySize/encodedBodySize) %>%
  # ratio of page load to encodedBodySize
  mutate(lt_encodedBodySize = loadtime/encodedBodySize) %>% 
  mutate(lt_resourceCount = loadtime/resourceCount) %>%
  # difference of responseStart
  mutate(lt_responseStart = loadtime-responseStart) %>% 
  mutate(lt_requestStart = loadtime-requestStart) %>%
  # ratio of page load to durations 
  mutate(lt_backendTime = loadtime/backendTime) %>% 
  mutate(lt_frontEndTime = loadtime/frontEndTime) %>% 
  mutate(lt_resourceDuration = loadtime/resourceDuration) %>%
  mutate(lt_serverResponseTime = loadtime/serverResponseTime) %>%
  # ratio of durations to encodedBodySize
  mutate(bet_encodedBodySize = backendTime/encodedBodySize) %>% 
  mutate(fet_encodedBodySize = frontEndTime/encodedBodySize) %>%
  mutate(rd_encodedBodySize = resourceDuration/encodedBodySize) %>%
  mutate(srt_encodedBodySize = serverResponseTime/encodedBodySize) %>%
  mutate(predictions = predict(lm.val, .))
```


# tl;dr
This works focused on determining an optimal linear regression model that predicts [`speedIndex`](https://sites.google.com/a/webpagetest.org/docs/using-webpagetest/metrics/speed-index) using a combination [navigation](https://docs.google.com/document/d/1W-EREsJLuRvTPvGXaW71FvuAGXkoLNDZmZl-sbaeMZM/edit#heading=h.q9b66ketjl7j) and resource metrics. This analysis is an extension of a [previous](https://metrics.mozilla.com/protected/cdowhygelund/visual_metrics_nav_timings.html) work, which includes additional covariates for modeling the [visual metrics](https://github.com/WPO-Foundation/visualmetrics). These specifically include resource and network metrics, such as `encodedBodySize` and `resourceCount`. It was found the relationship between visual and the other metrics have a strong dependency on URL, but a weak one on mode (e.g., application). The resultant model had a training leave-one-out cross-validation MAPE score of `r round(MAPE(cv$value, cv$response), 2)` and a score of `r round(MAPE(df.val$predictions, df.val$speedIndex), 2)` against a held-out validation set. This proof-of-concept work verifies the ability of producing a useful model that infers visual metrics method from navigation and resource metrics. To obtain a production model, it is recommended that significantly more URLs (100/1000x) are measured across a range of applications and platforms. 
  
# Datasets{.tabset}

Two datasets were provided in early 2020 by Andrew Creskey that contain in-house, live-site testing of a range of URLs for various mobile Firefox versions (`mode`) . Hereafter they referred to as drop 1 and 2. The datasets have identical measurements, save the inclusion of [`rumSpeedIndex`](https://github.com/WPO-Foundation/RUM-SpeedIndex) in drop 2, which is an effort to use resource timings as a proxy for `speedIndex`. In addition, the datasets differ in the mobile application versions, and URLs tested.

The same number of measurements with respect to URL and `mode` were taken relative to each dataset:

* Drop 1: `r df_0120 %>% count(mode, url) %>% pull(n) %>% max()` per URL and mode
* Drop 2: `r df_0220 %>% count(mode, url) %>% pull(n) %>% max()` per URL and mode

## Mode{#mode}

The `mode` field represents the Fenix, Fennec, or GeckoView version tested. 

```{r mode, echo=FALSE}
knitr::kable(data.frame(Mode = sort(unique(df$mode))) %>%
               mutate(`Drop 1` = Mode %in% df_0120$mode) %>%
               mutate(`Drop 2` = Mode %in% df_0220$mode) )
```

## URLs{#url}
A total of `r length(unique(df$url))` URLs were measured between the two drops, with `r length(unique(df_0120$url))` being measured in the first, and `r length(unique(df_0220$url))` in the second. 


```{r urls, echo=FALSE}
knitr::kable(
  data.frame(URL = sort(unique(df$url))) %>%
    mutate(`Drop 1` = URL %in% df_0120$url) %>%
    mutate(`Drop 2` = URL %in% df_0220$url) 
    )
```

## Visual Metrics
`r length(visual_metrics)` were measured.

```{r vis_metrics, echo=FALSE}
knitr::kable(df_status(df %>% select(visual_metrics), print_results=FALSE))
```

## Navigation and Resource Metrics

The [navigation](https://docs.google.com/document/d/1W-EREsJLuRvTPvGXaW71FvuAGXkoLNDZmZl-sbaeMZM/edit#heading=h.q9b66ketjl7j) and resource metrics can be categorized as follows:

Features that represent page load completion:

* `loadtime`
* `rumSpeedIndex`
* `domInteractive`
* `DOMContentLoaded`
* `firstPaint`

Feature representing starting point of backend activity:

* `connectStart`
* `domainLookupStart`
* `domainLookupEnd`
* `fetchStart`
* `requestStart`
* `responseStart`
* `responseEnd`

Features representing total duration of certain portions of backend activity:

* `backendTime`
* `domainLookupTime`
* `frontEndTime`
* `redirectionTime`
* `resourceDuration`
* `serverConnectionTime`
* `serverResponseTime`

Features representing resource usage:

* `decodedBodySize`
* `encodedBodySize`
* `resourceCount`

`r length(nav_timings)-1` navigation and resource metrics were measured for each drop. `r length(dropped_covars)` had a significant fraction of zero values, corresponding to empty measurements. Interestingly, these all involve measurements of certiain periods of  backend activity. These have been filtered from subsequent analysis (`r paste(dropped_covars, sep=', ')`).

```{r args, echo=FALSE}
knitr::kable(df_state)
```

# SpeedIndex{.tabset}
This effort focused on modeling `speedIndex`, as it is one of the oldest and popular visual metrics. Moreover, `rumSpeedIndex` is a corresponding effort at modeling this metric. 

The drop 2 dataset was only used for the modeling tasks. This ensured completeness in the dataset, such that as each mode was measured similarly for each URL. The drop 1 dataset is used as a [hold-out](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Holdout_dataset) validation set. 

## Mode

The following plot shows the distribution of speedIndex, as a ridge plot. The shading in the ridge plots represent the 0-20th, 20-40th, 40-60th, and 80-100th quintiles 

The distributions between builds of Fenix are very consistent. `fennec68` has a lower values for the lower two quintiles, and higher for the 5th. 

```{r speedIndex_d2, fig.width=8, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE}
df_0220_long_si <- df_0220 %>%
  select(mode, url, speedIndex, nav_timing_clean) %>%
  gather('nav_timing', 'nav_timing_value', -speedIndex, -mode, -url)

# ps = list()
#ps[['ridge']] <- 
ggplot(df_0220, aes(x=speedIndex, y=mode, fill = factor(stat(quantile)))) +
  stat_density_ridges(
    geom = "density_ridges_gradient", calc_ecdf = TRUE,
    quantiles = 5, quantile_lines = TRUE
  ) +
  scale_fill_viridis_d(name = "Quintiles") +
  theme(axis.text.y = element_text(angle = 25, hjust = 1)) +
  xlim(0, 10000)

# ps[['violin']] <- ggplot(df_0220, aes(mode, speedIndex)) +
#   geom_violin(draw_quantiles = c(0.20, 0.4, 0.60, 0.80)) +
#   theme(axis.text.x = element_text(angle = 20, hjust = 1))
# 
# plot_grid(plotlist = ps)
```

## Correlation{#corr}

```{r corr, echo=FALSE}
df_corr <- df_0220[, c(nav_timing_clean, 'speedIndex')]
cor_si <- cor(df_corr)

cor_si_cut <- as.data.frame(cor_si) %>% 
  tibble::rownames_to_column("nav_timing") %>% 
  select(nav_timing, speedIndex) %>% 
  filter(speedIndex > 0.5) %>% 
  arrange(desc(speedIndex)) %>%
  slice(-1)
```

The correlation of these visual metrics across all modes is shown in the following plot. The rightmost column contains `speedIndex`. `r length(cor_si_cut$nav_timing)` timings have correlations > 0.5: `r paste(cor_si_cut$nav_timing, sep=',')`. Interestingly, `rumSpeedIndex` has a low Pearson correlation with `speedIndex`.

A few immediate takeaways: 

*  For starting point timings:
    - `responseStart` and `responseEnd` are perfectly correlated to `backendTime`. 
    - `connectStart`, `domainLookupEnd`, `domainLookupStart`, `fetchStart` as perfectly correlated to `redirectionTime`.
*  For page load metrics
    - `loadtime` correlates the strongest with `speedIndex`.
    - `rumSpeedIndex` and `loadtime` have least correlation.


```{r corrplot, fig.width=10, fig.height=10, echo=FALSE, warning=FALSE}
corrplot::corrplot(cor_si, 
                   method = 'pie',
                   type = 'upper')# , order = 'hclust', addrect = 3) 
# corrplot(cor(df %>% select(-c(mode, url))), method = 'pie', type = 'upper') 
```

## Navigation and Resource Metrics{.tabset}
The following interactive plots display `speedIndex` versus the navigation and resource metrics. A points color represents [URL](#url), and shape represents [mode](#mode). There are a few clear takeaways:

* Relationships are strongly dependent upon `uri`.
* Relationships have much less dependency on `mode`.
   - However, `fennec68` does have cases were it is offset with respect to `fenix<>` measurements. 
* Several of the metrics, especially the page load timings, have a strong linear relationship with `speedIndex` (e.g., `rumSpeedIndex`, `loadTime`, `encodedBodySize`),
  - There are consistent outliers across the metrics (e.g.: https://www.nytimes.com, https://www.bing.com/search?q=restaurant).
* `decodedBodySize` and `encodedBodySize` cannot account simultaneously for the deviation in https://www.nytimes.com, and the large spread in https://www.bing.com/search?q=restaurant, with high values observed for https://accounts.google.com.
* Significant spread in the relationship to navigation and resource metrics for low `speedIndex` values. 

### Page Load Timings

The figures below compares `speedIndex` to the page load timings. 

```{r , fig.width=15, fig.height=40, echo=FALSE, warning=FALSE}

# ggplot(df_0220_long %>% 
#          filter(vis_metric == 'speedIndex'), 
#        aes(vis_metric_value, nav_timing_value)) +
#   geom_point(aes(shape=mode, color=url)) + 
#   facet_grid(vars(nav_timing), scales='free')

# plist <- list()
# for (i in 1:length(nav_timing_clean)){
#   nav_timing <- nav_timing_clean[i]
#   p <- ggplot(df_0220 %>%
#                 filter(DOMContentLoaded < 10000) %>%
#                 filter(resourceDuration < 500000),
#               aes_string(nav_timing, 'speedIndex')) +
#     geom_point(aes(color = url, shape=mode)) +
#     geom_smooth(method = "lm", se = FALSE) +
#     theme_bw()
#   if(i==22){
#     # plist[['legend']] <- get_legend(p + theme(legend.position = "top"))
#     plist[['legend']] <- get_legend(p + theme(legend.position = "right"))
#   }
#   p <- p + theme(legend.position = "none")
#   plist[[nav_timing]] <- p
# }
# 
# plot_grid(plotlist = plist, ncol=3)

# plist <- list()
# covar_grps <- split(nav_timing_clean, ceiling(seq_along(nav_timing_clean)/3))
# for (i in 1:length(covar_grps)){
#   grp <- covar_grps[i][[1]]
#   df_covar_grp <- df_0220_long %>%
#     filter(nav_timing %in% grp) %>%
#     filter(vis_metric == 'speedIndex')
#   p <- ggplot(df_covar_grp, aes(nav_timing_value, vis_metric_value)) +
#     geom_point(aes(color = url, shape=mode)) +
#     geom_smooth(method = "lm", se = FALSE) +
#     facet_grid(cols=vars(nav_timing), scales = 'free') +
#     theme_bw()
#   if(i==22){
#     # plist[['legend']] <- get_legend(p + theme(legend.position = "top"))
#     plist[['legend']] <- get_legend(p + theme(legend.position = "right"))
#   }
#   p <- p + theme(legend.position = "none")
#   plist[[i]] <- p
# }
# 
# plot_grid(plotlist = plist, ncol=1)
```

```{r page_load_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
# out.width='100%', out.height='100%', echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% pl_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') +
  ylab('SpeedIndex'))  #%>%
  #layout(autosize = F, autosize = F, width = 800, height = 500)
```

### Backend Timings

The figures below compares `speedIndex` to the starting point of backend activity timings. 

```{r backend_timing_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% bckend_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') + 
  ylab('SpeedIndex')) #%>%
  #layout(autosize = F, autosize = F, width = 800, height = 500)
```

### Backend Durations

The figures below compares `speedIndex` to the duration of background activity timings. 

```{r backend_dur_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% dur_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') + 
  ylab('SpeedIndex')) # %>%
  #layout(autosize = F, autosize = F, width = 800, height = 500)
```

### Resource Usage

The figures below compares `speedIndex` to resource usage metrics. 

```{r resource_si_plts, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(df_0220_long_si %>%
             filter(nav_timing %in% res_covars), aes(nav_timing_value, speedIndex)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_grid(cols=vars(nav_timing), scales = 'free') +
  theme_bw() + 
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  xlab('') + 
  ylab('SpeedIndex')) # %>%
  # layout(autosize = F, autosize = F, width = 800, height = 500)
```

# Process
The modeling effort focused on best representing `speedIndex` using the navigation and resource metrics for the 2nd dataset drop containing `rumSpeedIndex`. The steps taken are as follows: 

1. Review correlations between fields and `speedIndex`.
2. Generate new, potentially informative, features from the original covariates. 
3. Filter out uninformative and unimportant features.
4. Perform algorithmic feature selection to further narrow covariate space. Keep top _N_ features. 
5. Perform leave-one-out cross validation with respect to unique URIs. Train simple linear regression models. 
6. Validate model performance against the drop 1 dataset. 

## Feature Filtering
Redundant and uninformative features were manually filtered, in order to increase model performance, and to keep the feature selection search space tractable. The following filtering steps were performed:

* `responseStart` and `responseEnd` were dropped as they are perfectly correlated to `backendTime`.
* `connectStart`, `domainLookupEnd`, `domainLookupStart`, `fetchStart` were dropped as they are perfectly correlated to `redirectionTime`.
* The page load timings `domInteractive`, `DOMContentLoaded`, `firstPaint` were filtered:
    - `loadtime` correlates strongest with `speedIndex`.
    - `rumSpeedIndex` has good modeling properties on visual inspection.
    - `rumSpeedIndex` and `loadtime` have least correlation between page load metrics.
* `decodedBodySize` is dropped as it is very similar to `encodedBodySize`. 

##  Feature Generation

Features were generated using [page load cycle](https://docs.google.com/document/d/1W-EREsJLuRvTPvGXaW71FvuAGXkoLNDZmZl-sbaeMZM/edit#heading=h.q9b66ketjl7j) definitions,  referring to efforts of this [bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1615369), and the results of the [correlation](#corr) and exploratory analyses. The general idea was to focus on the page load covariates, and use the remaining timings, durations, and resource usage metrics as corrections to the model. 

* Ratio of `decodedBodySize` to `encodedBodySize`: 
    - `ratioDeEnCoded`
* Ratio of `resourceCount` to `encodedBodySize`: 
    - `ratioRescountEncoded`
* Ratio of `loadtime` to `resourceCount` and `encodedBodySize`: 
    - `lt_resourceCount`, `lt_encodedBodySize`
* Ratio of `rumSpeedIndex` to `resourceCount` and `encodedBodySize`: 
    - `rsi_resourceCount`, `rsi_encodedBodySize`
* Difference of `responseStart`, `requestStart` to `loadtime`:
    - `lt_responseStart`, `lt_requestStart`
* Difference of `responseStart`, `requestStart` to `rumSpeedIndex`:
    - `rsi_responseStart`
    - `rsi_requestStart`
* ratio of `loadtime` to durations:
    - `lt_backendTime`, `lt_frontEndTime`, `lt_resourceDuration`, `lt_serverResponseTime`
* ratio of `rumSpeedIndex` to durations:
    - `rsi_backendTime`, `rsi_frontEndTime`, `rsi_resourceDuration`, `rsi_serverResponseTime`
* ratio of durations to `encodedBodySize`:
  - `bet_encodedBodySize`, `fet_encodedBodySize`, `rd_encodedBodySize`, `srt_encodedBodySize`
  - `encodedBodySize` is not highly correlated to any durations.

# Variable Importance{.tabset}

To limit the number of covariates utilized in model training, thereby minimizes the risk of overfitting, feature selection was performed on the complete covariate set. There are significant number of methods for determining variable importance. The Two explored in this effort are using [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline) and and the R library Boruta.

## Mars
Multiple adaptive regression splines (MARS) is a form of regression that is an extension of multiple linear regressions and generalized linear models.  This algorithm can be used for variable importance, as the contribution  (e.g., variable importance  score) for each covariate can be calculated using generalized cross-validation (GCV) statistics. The table below shows the selected covariates after feature selection by MARS. 

```{r mars_featurs, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(as.data.frame(unclass(ev[,c(3,4,6)])) %>%
               tibble::rownames_to_column(., "covariate") %>%
               select(covariate, gcv)
             )
```


## Boruta
[Boruta](https://cran.r-project.org/web/packages/Boruta/index.html) is a feature selection algorithm that trains suites of Random Forest models to determine variable importance for regression and classification tasks. The following table shows the variable importance measures found for each navigation timing.  

```{r boruta, echo=FALSE, warning=FALSE, message=FALSE}
speedIndex.bor <- Boruta(speedIndex ~ ., data = df_0220_ultra_f %>% 
                           select(-c(mode, url)),
                         doTrace = TRUE, ntree = 500, maxRuns = 1000)

speedIndex.bor.df <- attStats(speedIndex.bor) %>% 
  tibble::rownames_to_column(., 'covariate') %>% 
  arrange(desc(medianImp)) %>%
  select(covariate, meanImp, medianImp)

knitr::kable(speedIndex.bor.df)
```



# Modeling{.tabset}

[Leave-one-out cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) was performed across the URLs to determine model performance. This is due to the strong dependency of the relationship between `speedIndex` and the navigation and resource metrics on URL. LOO CV across URL will yield the best estimation of model performance on unseen URLs.  The mean absolute percentage error loss (MAPE) was calculated from the resultant predictions for each held-out URL.  

### Mars Features

CV was performed on a model trained with the top 4 features found by MARS, with a final MAPE and R2 scores of `r round(MAPE(cv$value, cv$response), 2)` and `r round(R2(cv$value, cv$response), 2)`.

```{r lm_loo_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(cv, aes(value, response))+
  geom_point(aes(color = url, shape=mode)) +
  geom_abline(slope = 1) + 
  geom_abline(slope = 0) + 
  # facet_grid(rows = vars(metric), scales = 'free_y') +
  theme_bw() + 
  theme(legend.position = "none")+
        # axis.text.x = element_text(angle = 90, hjust = 1)) +
  #guides(color = guide_legend(ncol=2, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  ylab(response) +
  xlab('Predictions')
  )
```

### Boruta Features
```{r lm_loo_boruta, echo=FALSE, warning=FALSE}
cv <- cv_lm_loo(df_0220_ultra_f, 
                      formula = speedIndex ~ rumSpeedIndex + rsi_requestStart + srt_encodedBodySize + loadtime + rsi_responseStart,
                      response) %>%
    gather('metric', 'value', -response, -url, -mode, -fold)
```

CV was performed on a model trained with the top 4 features found by Boruta, with a final MAPE and R2 scores of `r round(MAPE(cv$value, cv$response), 2)` and `r round(R2(cv$value, cv$response), 2)`. Interestingly, Boruta's selected feature perform significantly worse than MARS, which could be due to using decision trees instead of linear regression models. 

```{r lm_loo_boruta_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(cv, aes(value, response)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_abline(slope = 1) + 
  geom_abline(slope = 0) + 
  # facet_grid(rows = vars(metric), scales = 'free_y') +
  theme_bw() + 
  theme(legend.position = "none")+
        # axis.text.x = element_text(angle = 90, hjust = 1)) +
  #guides(color = guide_legend(ncol=2, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  ylab(response) +
  xlab('Predictions')
  )
```


# Model Validation{.tabset}

The final modeling step is to calculate model performance using a held-out dataset. For this purposes the drop 1 dataset was used.

## Model Fit
The drop 1 dataset doesn't contain `rumSpeedIndex`, which is one of top 4 features found by MARS. Therefore, feature selection was performed again, subtracting this feature and any of its derivatives:

The results of the MARS algorithm:
```{r mars_val, echo=FALSE, warning=FALSE, error=FALSE}
library(earth)
speedIndex.mars.val <- earth(speedIndex ~ ., data = df_0220_ultra_f %>% 
                           select(-c(mode, url, rumSpeedIndex, starts_with('rsi'))))
ev.val <- evimp (speedIndex.mars.val)
knitr::kable(as.data.frame(unclass(ev.val[,c(3,4,6)])) %>%
               tibble::rownames_to_column(., "covariate") %>%
               select(covariate, gcv)
             )
```


```{r loo_cv_val, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
cv <- cv_lm_loo(df_0220_ultra_f, 
                      formula = speedIndex ~ loadtime + ratioDeEnCoded + lt_encodedBodySize + resourceCount,
                      response) %>%
    gather('metric', 'value', -response, -url, -mode, -fold)

```

LOO CV using these features yeilds a MAPE and R2 scores of `r round(MAPE(cv$value, cv$response), 2)` and `r round(R2(cv$value, cv$response), 2)`, respectively. The MAPE score is higher than that found with features including `rumSpeedIndex`, suggesting weaker model performance and a poorer resultant fit. 

```{r loo_cv_val_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}
ggplotly(ggplot(cv, aes(value, response)) +
  geom_point(aes(color = url, shape=mode)) +
  geom_abline(slope = 1) + 
  geom_abline(slope = 0) + 
  # facet_grid(rows = vars(metric), scales = 'free_y') +
  theme_bw() + 
  theme(legend.position = "none")+
        # axis.text.x = element_text(angle = 90, hjust = 1)) +
  #guides(color = guide_legend(ncol=2, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
  ylab(response) +
  xlab('Predictions')
  )
```

## Validation

The validation MAPE and R2 scores are `r round(MAPE(df.val$predictions, df.val$speedIndex), 2)` and `r round(R2(df.val$predictions, df.val$speedIndex), 2)`, respectively. There is a strong dependency of model performance with URL, as noted previously. https://imgur.com is an especially pathological case.

```{r lm_val_model_fit_plt, fig.width=5, fig.height=5, echo=FALSE, warning=FALSE}

ggplotly(
    ggplot(df.val, aes(predictions, speedIndex))+
    geom_point(aes(color = url, shape=mode)) +
    geom_abline(slope = 1) + 
    geom_abline(slope = 0) + 
    # facet_grid(rows = vars(metric), scales = 'free_y') +
    theme_bw() + 
    theme(legend.position = "none") +
    ylab(response) + 
      xlab('predictions')
)
```

# Visual Metrics{.tabset}

The focus of the modeling tasks was on `speedIndex`. However, there were `r length(visual_metrics)` measured in the two datasets. The following plots show the relationships between the various visual metrics and the top covariates found by MARS. In general, the trends observed for each URL hold across the various visual metrics. Therefore, the choice of visual metric for modeling appears to be of less significance, than the variety of URLs measured.  

## `rumSpeedIndex` Features
```{r vis_met_comp_1, fig.width=10, fig.height=10, echo=FALSE}
df_0220_ultra_vis_long <- df_0220_ultra_vis %>%
  gather('vis_metric', 'vis_metric_value', visual_metrics) %>%
  gather('nav_timing', 'nav_timing_value', -vis_metric, -vis_metric_value, -mode, -url)

ggplotly(
  ggplot(df_0220_ultra_vis_long %>%
                filter(nav_timing %in% rownames(ev[1:4, ])),
                aes(nav_timing_value, vis_metric_value)) +
    facet_grid(vars(vis_metric), vars(nav_timing), scales='free') + 
    geom_point(aes(color = url, shape=mode)) +
    geom_smooth(method = "lm", se = FALSE) + 
    theme_bw() + 
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 90, hjust = 1)) +
    guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
    xlab('Navigation Timing Value') + 
    ylab('Visual Metric Value')
)
```

## `loadtime` Features
```{r vis_met_comp_2, fig.width=10, fig.height=10, echo=FALSE}
ggplotly(
  ggplot(df_0220_ultra_vis_long %>%
                filter(nav_timing %in% rownames(ev.val[1:4, ])),
                aes(nav_timing_value, vis_metric_value)) +
    facet_grid(vars(vis_metric), vars(nav_timing), scales='free') + 
    geom_point(aes(color = url, shape=mode)) +
    geom_smooth(method = "lm", se = FALSE) + 
    theme_bw() + 
    theme(legend.position = "none",
          axis.text.x = element_text(angle = 90, hjust = 1)) +
    guides(color = guide_legend(nrow=5, byrow=TRUE), shape = guide_legend(nrow=4, byrow=TRUE)) +
    xlab('Navigation Timing Value') + 
    ylab('Visual Metric Value')
)
```

# Next Steps

The following steps should be taken to produce a production grade model for use in telemetry:

1. Determine the optimal visual metric for measuring in telemetry. This should best represent user perceived page load performance. 
2. Measure significantly more live URLs, with ~100 being a good starting point. 
3. Measure across desktop and mobile platforms. 
4. Utilize recursive feature selection, with resampling across URLs to determine optimal feature set.
5. Once a larger, more thorough dataset is obtained, examine additional regression modeling techniques. 

